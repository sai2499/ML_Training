{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "707ef5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#download'\n",
      "WARNING: You are using pip version 21.3.1; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\saina\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk #download nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce2b8076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package             Version\n",
      "------------------- ---------\n",
      "argon2-cffi         20.1.0\n",
      "async-generator     1.10\n",
      "attrs               21.2.0\n",
      "backcall            0.2.0\n",
      "bleach              3.3.0\n",
      "certifi             2021.10.8\n",
      "cffi                1.14.5\n",
      "charset-normalizer  2.0.7\n",
      "click               8.1.3\n",
      "colorama            0.4.4\n",
      "cycler              0.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\saina\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decorator           5.0.8\n",
      "defusedxml          0.7.1\n",
      "entrypoints         0.3\n",
      "et-xmlfile          1.1.0\n",
      "graphviz            0.16\n",
      "idna                3.3\n",
      "imageio             2.9.0\n",
      "ipykernel           5.5.5\n",
      "ipython             7.23.1\n",
      "ipython-genutils    0.2.0\n",
      "ipywidgets          7.6.3\n",
      "jedi                0.18.0\n",
      "Jinja2              3.0.0\n",
      "joblib              1.0.1\n",
      "jsonschema          3.2.0\n",
      "jupyter             1.0.0\n",
      "jupyter-client      6.1.12\n",
      "jupyter-console     6.4.0\n",
      "jupyter-core        4.7.1\n",
      "jupyterlab-pygments 0.1.2\n",
      "jupyterlab-widgets  1.0.0\n",
      "kiwisolver          1.3.1\n",
      "lxml                4.6.4\n",
      "MarkupSafe          2.0.0\n",
      "matplotlib          3.4.2\n",
      "matplotlib-inline   0.1.2\n",
      "mglearn             0.1.9\n",
      "mistune             0.8.4\n",
      "nbclient            0.5.3\n",
      "nbconvert           6.0.7\n",
      "nbformat            5.1.3\n",
      "nest-asyncio        1.5.1\n",
      "nltk                3.7\n",
      "notebook            6.3.0\n",
      "numpy               1.20.3\n",
      "openpyxl            3.0.9\n",
      "packaging           20.9\n",
      "pandas              1.2.4\n",
      "pandas-datareader   0.10.0\n",
      "pandocfilters       1.4.3\n",
      "parso               0.8.2\n",
      "pickleshare         0.7.5\n",
      "Pillow              8.2.0\n",
      "pip                 21.3.1\n",
      "prometheus-client   0.10.1\n",
      "prompt-toolkit      3.0.18\n",
      "pycparser           2.20\n",
      "Pygments            2.9.0\n",
      "pyparsing           2.4.7\n",
      "pyrsistent          0.17.3\n",
      "python-dateutil     2.8.1\n",
      "pytz                2021.1\n",
      "pywin32             300\n",
      "pywinpty            0.5.7\n",
      "pyzmq               22.0.3\n",
      "qtconsole           5.1.0\n",
      "QtPy                1.9.0\n",
      "regex               2022.6.2\n",
      "requests            2.26.0\n",
      "scikit-learn        0.24.2\n",
      "scipy               1.6.3\n",
      "seaborn             0.11.2\n",
      "Send2Trash          1.5.0\n",
      "setuptools          56.0.0\n",
      "six                 1.16.0\n",
      "terminado           0.9.5\n",
      "testpath            0.4.4\n",
      "threadpoolctl       2.1.0\n",
      "tornado             6.1\n",
      "tqdm                4.64.0\n",
      "traitlets           5.0.5\n",
      "urllib3             1.26.7\n",
      "wcwidth             0.2.5\n",
      "webencodings        0.5.1\n",
      "widgetsnbextension  3.5.1\n",
      "xlrd                2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ed698d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Saina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # nltk tokenization library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c4d9c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d64bcf5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentence Tokenization\n",
    "nltk.sent_tokenize(data)\n",
    "len(nltk.sent_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7b39146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem',\n",
       " 'Ipsum',\n",
       " 'is',\n",
       " 'simply',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'of',\n",
       " 'the',\n",
       " 'printing',\n",
       " 'and',\n",
       " 'typesetting',\n",
       " 'industry',\n",
       " '.',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'has',\n",
       " 'been',\n",
       " 'the',\n",
       " 'industry',\n",
       " \"'s\",\n",
       " 'standard',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'ever',\n",
       " 'since',\n",
       " 'the',\n",
       " '1500s',\n",
       " ',',\n",
       " 'when',\n",
       " 'an',\n",
       " 'unknown',\n",
       " 'printer',\n",
       " 'took',\n",
       " 'a',\n",
       " 'galley',\n",
       " 'of',\n",
       " 'type',\n",
       " 'and',\n",
       " 'scrambled',\n",
       " 'it',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'type',\n",
       " 'specimen',\n",
       " 'book',\n",
       " '.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'survived',\n",
       " 'not',\n",
       " 'only',\n",
       " 'five',\n",
       " 'centuries',\n",
       " ',',\n",
       " 'but',\n",
       " 'also',\n",
       " 'the',\n",
       " 'leap',\n",
       " 'into',\n",
       " 'electronic',\n",
       " 'typesetting',\n",
       " ',',\n",
       " 'remaining',\n",
       " 'essentially',\n",
       " 'unchanged',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'popularised',\n",
       " 'in',\n",
       " 'the',\n",
       " '1960s',\n",
       " 'with',\n",
       " 'the',\n",
       " 'release',\n",
       " 'of',\n",
       " 'Letraset',\n",
       " 'sheets',\n",
       " 'containing',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'passages',\n",
       " ',',\n",
       " 'and',\n",
       " 'more',\n",
       " 'recently',\n",
       " 'with',\n",
       " 'desktop',\n",
       " 'publishing',\n",
       " 'software',\n",
       " 'like',\n",
       " 'Aldus',\n",
       " 'PageMaker',\n",
       " 'including',\n",
       " 'versions',\n",
       " 'of',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word tokenization\n",
    "nltk.word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67263b7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Saina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('we', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('see', 'VB'),\n",
       " ('an', 'DT'),\n",
       " ('example', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('POS', 'NNP'),\n",
       " ('tagging', 'VBG')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "data = 'we will see an example of POS tagging'\n",
    "word_token = nltk.word_tokenize(data)\n",
    "nltk.pos_tag(word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8a14486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Saina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopword = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb83fd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['see', 'example', 'pos', 'tagging']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = []\n",
    "for word in nltk.word_tokenize(data.lower()):\n",
    "    if word not in stopword:\n",
    "        clean_data.append(word)\n",
    "clean_data\n",
    "'''\n",
    "1. Covert to text to lower case\n",
    "2. Tokenize\n",
    "3. remove Stopwords\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop words : a, an, the, at or etc\n",
    "#eg = \"I am a good boy\"\n",
    "'''\n",
    "Text processing is:\n",
    "1. word_tokenization\n",
    "2. Stop Words\n",
    "3. Punctuation\n",
    "4. Sentence with Clean Data.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "723e5b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "857562b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('see', 'VB'), ('example', 'NN'), ('pos', 'IN'), ('tagging', 'VBG')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compressed code for all steps\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "punct = string.punctuation\n",
    "clean_data = []\n",
    "\n",
    "for word in nltk.word_tokenize(data.lower()):\n",
    "    if word not in punct:\n",
    "        if word not in stopwords:\n",
    "            clean_data.append(word)\n",
    "nltk.pos_tag(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad378262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "# lancester - one of the fastest and complex algorithm\n",
    "# porter - one of the first and oldest method\n",
    "# snowball - porter2\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "308bd154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer\n",
      "hobbi\n",
      "hobbi\n",
      "comput\n",
      "comput\n",
      "histori\n",
      "histor\n",
      "lancaster Stemmer\n",
      "hobby\n",
      "hobby\n",
      "comput\n",
      "comput\n",
      "hist\n",
      "hist\n",
      "snowball Stemmer\n",
      "hobbi\n",
      "hobbi\n",
      "comput\n",
      "comput\n",
      "histori\n",
      "histor\n"
     ]
    }
   ],
   "source": [
    "print(\"Porter Stemmer\")\n",
    "print(porter.stem(\"hobby\"))\n",
    "print(porter.stem('hobbies'))\n",
    "print(porter.stem('computer'))\n",
    "print(porter.stem('computation'))\n",
    "print(porter.stem('history'))\n",
    "print(porter.stem('historical'))\n",
    "print(\"lancaster Stemmer\")\n",
    "print(lancaster.stem(\"hobby\"))\n",
    "print(lancaster.stem('hobbies'))\n",
    "print(lancaster.stem('computer'))\n",
    "print(lancaster.stem('computation'))\n",
    "print(lancaster.stem('history'))\n",
    "print(lancaster.stem('historical'))\n",
    "print(\"snowball Stemmer\")\n",
    "print(snowball.stem(\"hobby\"))\n",
    "print(snowball.stem('hobbies'))\n",
    "print(snowball.stem('computer'))\n",
    "print(snowball.stem('computation'))\n",
    "print(snowball.stem('history'))\n",
    "print(snowball.stem('historical'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d14fddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LancasterStemmer>\n",
      "i was going to the off on my bik when i saw a car pass by hit the tre\n",
      "<nltk.stem.snowball.SnowballStemmer object at 0x000001E21E890FA0>\n",
      "i was go to the offic on my bike when i saw a car pass by hit the tree\n",
      "<PorterStemmer>\n",
      "i wa go to the offic on my bike when i saw a car pass by hit the tree\n"
     ]
    }
   ],
   "source": [
    "#Why lancaster is not used:\n",
    "sent = \"I was going to the office on my bike when i saw a car passing by hit the tree\"\n",
    "token = list(nltk.word_tokenize(sent))\n",
    "for stemmer in (lancaster, snowball, porter):\n",
    "    print(stemmer)\n",
    "    stemm = [stemmer.stem(t) for t in token]\n",
    "    print(\" \".join(stemm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe6bf2e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Saina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Saina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7574b16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize(\"running\", pos = 'v')) # Pos is used to the verb or noun part of the word\n",
    "print(lemma.lemmatize(\"runs\",pos= 'v'))\n",
    "print(lemma.lemmatize(\"ran\",pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee41d325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lorem',\n",
       " 'ipsum',\n",
       " 'simply',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'print',\n",
       " 'typeset',\n",
       " 'industry',\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " 'industry',\n",
       " \"'s\",\n",
       " 'standard',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'ever',\n",
       " 'since',\n",
       " '1500s',\n",
       " 'unknown',\n",
       " 'printer',\n",
       " 'take',\n",
       " 'galley',\n",
       " 'type',\n",
       " 'scramble',\n",
       " 'make',\n",
       " 'type',\n",
       " 'specimen',\n",
       " 'book',\n",
       " 'survive',\n",
       " 'five',\n",
       " 'centuries',\n",
       " 'also',\n",
       " 'leap',\n",
       " 'electronic',\n",
       " 'typeset',\n",
       " 'remain',\n",
       " 'essentially',\n",
       " 'unchanged',\n",
       " 'popularise',\n",
       " '1960s',\n",
       " 'release',\n",
       " 'letraset',\n",
       " 'sheet',\n",
       " 'contain',\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " 'passages',\n",
       " 'recently',\n",
       " 'desktop',\n",
       " 'publish',\n",
       " 'software',\n",
       " 'like',\n",
       " 'aldus',\n",
       " 'pagemaker',\n",
       " 'include',\n",
       " 'versions',\n",
       " 'lorem',\n",
       " 'ipsum']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compressed code for all steps\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "string.punctuation\n",
    "data = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\"\n",
    "stopwords = stopwords.words('english')\n",
    "punct = string.punctuation\n",
    "clean_data = []\n",
    "\n",
    "for word in nltk.word_tokenize(data.lower()):\n",
    "    if word not in punct:\n",
    "        if word not in stopwords:\n",
    "            clean_data.append(word)\n",
    "nltk.pos_tag(clean_data)\n",
    "[lemma.lemmatize(i, pos = 'v') for i in clean_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630c955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
